# Prediction-of-Loan-Status

## Introduction

To loan or not to loan is the key question that all mortgage companies are asking themselves when a new client is seeking for a loan. The common sense is that the credit history, net worth, income and job stability all play important roles in determining if the client can pay the loan off or default. This project targets on building models to predict the final loan status and find out the most important features. Random Forest, XGBoost, and LightGBM are used. It was found that the LightGBM performs the best with the train AUC of 0.7889 and the test AUC of 0.7668. The most important feature that drives all the three models the most was found to be the interest rate. The data used is originally from https://www.lendingclub.com/. I downloaded the historical loan data from 2007 to 2018 from https://www.kaggle.com/wordsforthewise/lending-club. This data set is preferred because it has a large amount of records that enables training a reliable model. And each applicant has been followed up through out the whole loan period and the loan status is final. I sliced the data from 2012 to 2018 for this analysis. The dataset has 2215531 rows and 151 columns.

## Feature Selection

![Image description](https://github.com/hanzheng0730/Prediction-of-Loan-Status/blob/master/Correlation%20Matrix.jpg)

Correlation matrix was generated and used to check the correlation between the features. For the two highly correlated features, only one of them was kept. I also used the correlation matrix to identify a few more posterior features that were obtained by monitoring the payment activities of the borrower, and therefore, these features were removed as well. The features include: ’out_prncp’, ’out_prncp_inv’, ’total_pymnt’, ’total_pymnt_inv’, ’total_rec_prncp’, ’total_rec_late_fee’, ’last_fico_range_high’, ’last_fico_range_low’, ’grade’, ’total_rec_int’. As a result, the total number of features used for the modeling became 57.

## Results

RandomForestClassifier, XGBClassifier and LGBMClassifier have been selected to train the classification models. I split the variable set into train set and test set by 4:1. The RandomForestClassifier using default parameters gave the train AUC of 0.9997 and the test AUC of 0.6347 which is obviously overfit. Using RandomizedSearchCV on the RandomForestClassifier gave the train AUC of 0.6927 and the test AUC of 0.6903 which is neither overfit nor underfit. It can be seen that the boundaries values of the grid were selected for ’n_estimators’, ’min_samples_split’, ’min_samples_leaf’. The top 3 important features for Random Forest model are: ’int_rate’, ’fico_range_low’, ’term_le’.

The XGBClassifier using default parameters gave the train AUC of 0.7442 and the test AUC of 0.7409 which is neither overfit nor underfit. Using RandomizedSearchCV on the XGBClassifier gave the train AUC of 0.6790 and the test AUC of 0.6787 which is neither overfit nor underfit. The top 3 important features for the XGBoost model are: ’int_rate’, ’loan_amnt’, ’annual_inc’. It is in line with the Random Forest results that the ’int_rate’ is the most important feature. 

The best performance has been found for the LGBMClassifier with the default parameter. It gave the train AUC of 0.7889 and the test AUC of 0.7668 which is neither overfit nor underfit. Similarly, using the Randomized-SearchCV on the LGBMClassifier deteriorated the accuracy to 0.7487 for the train set and to 0.7455 for the test set. The parameter ’max_depth’ was picked at the boundary value of the given grid. For the future work, I will work on a wider and denser grid. I used SHAP for explaining the feature importance of this best model. The top 3 important features have been found as: ’int_rate’, ’acc_open_past_24mths’, ’loan_amnt’. It was found that the second and third important features differ between models but the most important feature is always the ’int_rate’.

## Discussions

Parameter tuning methods like GridSearchCV and Hyperopt have been tried but they did not complete after running overnight. This dataset has over two million records which is considered to be very large. This work demonstrates that GridSearchCV and Hyperopt are not proper for the large data set, while the RandomizedSearchCV can be considered more proper. However, I would suggest using a relatively wide and dense grid for the RandomizedSearchCV in order to get the optimized result.
The ’int_rate’ has been found to be the most important feature that drives the loan status the most. And this finding is consistent across the models. Besides that, ’fico_range_low’, ’term_le’, ’loan_amnt’, ’annual_inc’, ’acc_open_past_24mths’ are also important for evaluating a loan applicant. This provides insights for mortgage underwriters to make the right judgement. It is believed that the result based on this large enough data set are more reliable than those based on the small data set. Another thing that I will work on as a future work is to try ensemble models by assigning different weights to different models.
